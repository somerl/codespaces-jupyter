{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population Data from CSV\n",
    "\n",
    "This notebooks reads sample population data from `data/atlantis.csv` and plots it using Matplotlib. Edit `data/atlantis.csv` and re-run this cell to see how the plots change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ray[rllib] in /opt/python/3.10.8/lib/python3.10/site-packages (2.6.1)\n",
      "Requirement already satisfied: tensorflow in /opt/python/3.10.8/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from ray[rllib]) (8.1.6)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from ray[rllib]) (3.12.2)\n",
      "Requirement already satisfied: jsonschema in /home/codespace/.local/lib/python3.10/site-packages (from ray[rllib]) (4.18.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from ray[rllib]) (1.0.5)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.10/site-packages (from ray[rllib]) (23.1)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/python/3.10.8/lib/python3.10/site-packages (from ray[rllib]) (4.23.4)\n",
      "Requirement already satisfied: pyyaml in /home/codespace/.local/lib/python3.10/site-packages (from ray[rllib]) (6.0)\n",
      "Requirement already satisfied: aiosignal in /opt/python/3.10.8/lib/python3.10/site-packages (from ray[rllib]) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /opt/python/3.10.8/lib/python3.10/site-packages (from ray[rllib]) (1.4.0)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from ray[rllib]) (2.31.0)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from ray[rllib]) (1.56.2)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/python/3.10.8/lib/python3.10/site-packages (from ray[rllib]) (1.24.2)\n",
      "Requirement already satisfied: pandas in /opt/python/3.10.8/lib/python3.10/site-packages (from ray[rllib]) (1.5.3)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /opt/python/3.10.8/lib/python3.10/site-packages (from ray[rllib]) (2.6.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.1 in /opt/python/3.10.8/lib/python3.10/site-packages (from ray[rllib]) (12.0.1)\n",
      "Requirement already satisfied: dm-tree in /opt/python/3.10.8/lib/python3.10/site-packages (from ray[rllib]) (0.1.8)\n",
      "Requirement already satisfied: gymnasium==0.26.3 in /opt/python/3.10.8/lib/python3.10/site-packages (from ray[rllib]) (0.26.3)\n",
      "Requirement already satisfied: lz4 in /opt/python/3.10.8/lib/python3.10/site-packages (from ray[rllib]) (4.3.2)\n",
      "Requirement already satisfied: scikit-image in /opt/python/3.10.8/lib/python3.10/site-packages (from ray[rllib]) (0.21.0)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.10/site-packages (from ray[rllib]) (1.11.1)\n",
      "Requirement already satisfied: typer in /opt/python/3.10.8/lib/python3.10/site-packages (from ray[rllib]) (0.9.0)\n",
      "Requirement already satisfied: rich in /opt/python/3.10.8/lib/python3.10/site-packages (from ray[rllib]) (13.5.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from gymnasium==0.26.3->ray[rllib]) (2.2.1)\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in /opt/python/3.10.8/lib/python3.10/site-packages (from gymnasium==0.26.3->ray[rllib]) (0.0.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->ray[rllib]) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->ray[rllib]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/3.10.8/lib/python3.10/site-packages (from requests->ray[rllib]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->ray[rllib]) (2023.5.7)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema->ray[rllib]) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema->ray[rllib]) (2023.6.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema->ray[rllib]) (0.29.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema->ray[rllib]) (0.8.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas->ray[rllib]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas->ray[rllib]) (2023.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from rich->ray[rllib]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from rich->ray[rllib]) (2.15.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-image->ray[rllib]) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-image->ray[rllib]) (10.0.0)\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/python/3.10.8/lib/python3.10/site-packages (from scikit-image->ray[rllib]) (2.31.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/python/3.10.8/lib/python3.10/site-packages (from scikit-image->ray[rllib]) (2023.7.18)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/python/3.10.8/lib/python3.10/site-packages (from scikit-image->ray[rllib]) (1.4.1)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in /opt/python/3.10.8/lib/python3.10/site-packages (from scikit-image->ray[rllib]) (0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/python/3.10.8/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/python/3.10.8/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/python/3.10.8/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->ray[rllib]) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/python/3.10.8/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"ray[rllib]\" tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in /opt/python/3.10.8/lib/python3.10/site-packages (4.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[box2d] in /opt/python/3.10.8/lib/python3.10/site-packages (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from gymnasium[box2d]) (1.24.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from gymnasium[box2d]) (2.2.1)\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in /opt/python/3.10.8/lib/python3.10/site-packages (from gymnasium[box2d]) (0.0.1)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /opt/python/3.10.8/lib/python3.10/site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame==2.1.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from gymnasium[box2d]) (2.1.0)\n",
      "Requirement already satisfied: swig==4.* in /opt/python/3.10.8/lib/python3.10/site-packages (from gymnasium[box2d]) (4.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/python/3.10.8/lib/python3.10/site-packages (0.15.7)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/python/3.10.8/lib/python3.10/site-packages (from wandb) (8.1.6)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from wandb) (1.28.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /home/codespace/.local/lib/python3.10/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: pathtools in /opt/python/3.10.8/lib/python3.10/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /opt/python/3.10.8/lib/python3.10/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.10/site-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/python/3.10.8/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from wandb) (4.23.4)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/codespace/.local/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/3.10.8/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "#мы заменили float и np.array\n",
    "class GoLeftEnv(gym.Env):\n",
    "  \"\"\"\n",
    "  Custom Environment that follows gym interface.\n",
    "  This is a simple env where the agent must learn to go always left.\n",
    "  \"\"\"\n",
    "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "  metadata = {'render.modes': ['console']}\n",
    "  # Define constants for clearer code\n",
    "  FIRST = 0\n",
    "  SECOND = 1\n",
    "  THIRD = 2\n",
    "  FORTH = 3\n",
    "  FIFTH= 4\n",
    "  SIXTH = 5\n",
    "  SEVENTH = 6\n",
    "\n",
    "  def __init__(self, grid_size=10, render_mode=\"console\"):\n",
    "    super(GoLeftEnv, self).__init__()\n",
    "    self.render_mode = render_mode\n",
    "\n",
    "    self.fields = np.array([[0, 0, 7, 7,], [1, 0, 0, 0,], [2, 0, 0, 0,], [3, 0, 0, 0,], [4, 1, 0, 0,], [5, 0, 0, 0], [6, 0, 0, 0], [7, 0, 0, 0], [8, 1, 0, 0], [9, 0, 0, 0], [10, 0, 0, 0], [11, 0, 0, 0], [12, 0, 0, 0],\n",
    "                   [13, 0, 0, 0], [14, 1, 0, 0], [15, 0, 0, 0], [0, 0, 0, 0]])\n",
    "    \n",
    "\n",
    "    self.fields = self.fields.flatten('C')\n",
    "\n",
    "    self.fields = [x/16 for x in self.fields]\n",
    "\n",
    "    # Define action and observation space\n",
    "    # They must be gym.spaces objects\n",
    "    # Example when using discrete actions, we have two: left and right\n",
    "    n_actions = 7\n",
    "    self.action_space = spaces.Discrete(n_actions)\n",
    "    # The observation will be the coordinate of the agent\n",
    "    # this can be described both by Discrete and Box space\n",
    "    self.observation_space = spaces.Box(low=0, high=1,\n",
    "                                        shape=(68,), dtype=np.float32)\n",
    "  #попробовать переделать всё в np.array\n",
    "\n",
    "  def reset(self, seed=None, options=None):\n",
    "    \"\"\"\n",
    "    Important: the observation must be a numpy array\n",
    "    :return: (np.array)\n",
    "    \"\"\"\n",
    "    super().reset(seed=seed, options=options)\n",
    "\n",
    "\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.fields = np.array([[0, 0, 7, 7,], [1, 0, 0, 0,], [2, 0, 0, 0,], [3, 0, 0, 0,], [4, 1, 0, 0,], [5, 0, 0, 0], [6, 0, 0, 0], [7, 0, 0, 0], [8, 1, 0, 0], [9, 0, 0, 0], [10, 0, 0, 0], [11, 0, 0, 0], [12, 0, 0, 0],\n",
    "                   [13, 0, 0, 0], [14, 1, 0, 0], [15, 0, 0, 0], [0, 0, 0, 0]])\n",
    "\n",
    "\n",
    "    self.fields = self.fields.flatten('C')\n",
    "\n",
    "    self.fields = [x/16 for x in self.fields]\n",
    "\n",
    "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "    return np.array(self.fields).astype(np.float32), {}\n",
    "\n",
    "  def step(self, action):\n",
    "     \n",
    "     self.fields = [x if x == 0 else int(x*16) for x in self.fields]\n",
    "\n",
    "     self.fields = [int(a) for a in self.fields]\n",
    "\n",
    "     #print(self.fields)\n",
    "\n",
    "     self.fields = np.array(self.fields)\n",
    "\n",
    "     self.fields = self.fields.reshape(17, 4)\n",
    "\n",
    "     #print(self.fields)\n",
    "\n",
    "     def first():\n",
    "            numberforcycles = 15\n",
    "\n",
    "            balcycle1 = 0\n",
    "\n",
    "            roll = self.fields[16][0]\n",
    "\n",
    "            lst = [0, 1, 2, 3, 4]\n",
    "            weights = [6.25, 25, 37.5, 25, 6.25]\n",
    "            nextroll = random.choices(lst, weights=weights, k=1)\n",
    "            nextroll = nextroll[0]\n",
    "            self.fields[16][0] = nextroll\n",
    "            #print(\"Выпала\" , roll)\n",
    "            arrplayer1 = []\n",
    "            while balcycle1 < numberforcycles:\n",
    "              ballnow = self.fields[balcycle1][2]\n",
    "              ballindex = self.fields[balcycle1][0]\n",
    "          #расположение фишек игрока 1\n",
    "              if ballnow != 0:\n",
    "               arrplayer1.append(ballindex)\n",
    "              balcycle1 += 1\n",
    "            if self.fields[8][3] == 1:\n",
    "                arrplayer1.append(8)\n",
    "            #print(arrplayer1)\n",
    "\n",
    "            balcycle1 = 0\n",
    "            arrchoice1 = []\n",
    "            while balcycle1 < numberforcycles:\n",
    "              ballnow = self.fields[balcycle1][2]\n",
    "              ballindex = self.fields[balcycle1][0]\n",
    "              if ballnow != 0:\n",
    "                ballfuture = ballindex + roll\n",
    "                if ballfuture <= numberforcycles:\n",
    "                 if ballfuture not in arrplayer1:\n",
    "                  arrchoice1.append(ballfuture)\n",
    "              # print(\"Выпала\" , self.roll)\n",
    "                  #print(\"текущая позиция\" , ballindex)\n",
    "                  #print(\"возможная позиция\" , ballfuture)\n",
    "              balcycle1 += 1\n",
    "\n",
    "\n",
    "\n",
    "            if arrchoice1:\n",
    "\n",
    "              #случайный бот:\n",
    "              #playeronemove = random.choice(arrchoice1)\n",
    "\n",
    "              #Очень жадный бот:\n",
    "              playeronemove = len(arrchoice1)\n",
    "              playeronemove = arrchoice1[playeronemove-1]\n",
    "\n",
    "              #Жадный бот:\n",
    "              #playeronemove = 33\n",
    "              #for arrchoice in arrchoice1:\n",
    "              #  if self.fields[arrchoice][1] == 1 or (self.fields[arrchoice][3] == 1 and self.fields[arrchoice][0] >= 5 and self.fields[arrchoice][0] <= 12):\n",
    "              #    playeronemove = self.fields[arrchoice][0]\n",
    "              #if playeronemove == 33:\n",
    "              #  playeronemove = len(arrchoice1)\n",
    "              #  playeronemove = arrchoice1[playeronemove-1]\n",
    "\n",
    "              #playeronemove = random.choice(arrchoice1)\n",
    "              playeronepos = playeronemove - roll\n",
    "              #print(playeronepos)\n",
    "              self.fields[playeronepos][2] = self.fields[playeronepos][2] - 1\n",
    "              self.fields[playeronemove][2] = self.fields[playeronemove][2] + 1\n",
    "              if self.fields[playeronemove][3] == 1 and playeronemove >= 5 and playeronemove <= 12:\n",
    "                self.fields[playeronemove][3] = 0\n",
    "                self.fields[0][3] = self.fields[0][3] + 1\n",
    "                #print(\"мы забрали шашку на\" , self.fields[playeronemove][0])\n",
    "              #print(\"боту выпал\", roll)\n",
    "              #print(\"бот ходит\", playeronemove)\n",
    "              return playeronemove\n",
    "\n",
    "     def second():\n",
    "            numberforcycles = 15\n",
    "\n",
    "            agentroll = self.fields[16][1]\n",
    "\n",
    "\n",
    "            lst = [0, 1, 2, 3, 4]\n",
    "            weights = [6.25, 25, 37.5, 25, 6.25]\n",
    "            nextagentroll = random.choices(lst, weights=weights, k=1)\n",
    "            nextagentroll = nextagentroll[0]\n",
    "            self.fields[16][1] = nextagentroll\n",
    "            #print(\"Агенту Выпал\" , agentroll)\n",
    "\n",
    "            agentcycle = 0\n",
    "            arrplayeragent = []\n",
    "\n",
    "            while agentcycle < numberforcycles:\n",
    "              agentballnow = self.fields[agentcycle][3]\n",
    "              agentballindex = self.fields[agentcycle][0]\n",
    "          #расположение фишек игрока 2\n",
    "              if agentballnow != 0:\n",
    "               arrplayeragent.append(agentballindex)\n",
    "              agentcycle += 1\n",
    "            if self.fields[8][2] == 1:\n",
    "                arrplayeragent.append(8)\n",
    "\n",
    "            #print(\"фишки агента\", arrplayeragent)\n",
    "\n",
    "            agentcycle = 0\n",
    "            arrplayeragent2 = []\n",
    "\n",
    "            while agentcycle < numberforcycles:\n",
    "              agentballnow = self.fields[agentcycle][3]\n",
    "              agentballindex = self.fields[agentcycle][0]\n",
    "          #будущее фишек игрока 2\n",
    "              if agentballnow != 0:\n",
    "               agentballfuture = agentballindex + agentroll\n",
    "               if agentballfuture <= numberforcycles:\n",
    "                if agentballfuture not in arrplayeragent:\n",
    "                  arrplayeragent2.append(agentballfuture)\n",
    "              agentcycle += 1\n",
    "\n",
    "            #print(arrplayeragent2)\n",
    "\n",
    "\n",
    "\n",
    "            if arrplayeragent2:\n",
    "              playeragentmove = len(arrplayeragent2)\n",
    "              playeragentmove = arrplayeragent2[playeragentmove-1]\n",
    "\n",
    "              if action == self.FIRST:\n",
    "                if len(arrplayeragent2) > 0:\n",
    "                 playeragentmove = arrplayeragent2[0]\n",
    "                else:\n",
    "                 playeragentmove = playeragentmove\n",
    "              elif action == self.SECOND:\n",
    "               if len(arrplayeragent2) > 1:\n",
    "                playeragentmove = arrplayeragent2[1]\n",
    "               else:\n",
    "                playeragentmove = playeragentmove\n",
    "              elif action == self.THIRD:\n",
    "               if len(arrplayeragent2) > 2:\n",
    "                playeragentmove = arrplayeragent2[2]\n",
    "               else:\n",
    "                playeragentmove = playeragentmove\n",
    "              elif action == self.FORTH:\n",
    "               if len(arrplayeragent2) > 3:\n",
    "                playeragentmove = arrplayeragent2[3]\n",
    "               else:\n",
    "                playeragentmove = playeragentmove\n",
    "              elif action == self.FIFTH:\n",
    "               if len(arrplayeragent2) > 4:\n",
    "                playeragentmove = arrplayeragent2[4]\n",
    "               else:\n",
    "                playeragentmove = playeragentmove\n",
    "              elif action == self.SIXTH:\n",
    "               if len(arrplayeragent2) > 5:\n",
    "                playeragentmove = arrplayeragent2[5]\n",
    "               else:\n",
    "                playeragentmove = playeragentmove\n",
    "              elif action == self.SEVENTH:\n",
    "               if len(arrplayeragent2) > 6:\n",
    "                playeragentmove = arrplayeragent2[6]\n",
    "               else:\n",
    "                playeragentmove = playeragentmove\n",
    "              else:\n",
    "                raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "\n",
    "\n",
    "\n",
    "              #print(\"Агент делает выбор\", playeragentmove)\n",
    "\n",
    "              #playeragentmove = random.choice(arrplayeragent)\n",
    "              playeragentpos = playeragentmove - agentroll\n",
    "              self.fields[playeragentpos][3] = self.fields[playeragentpos][3] - 1\n",
    "              self.fields[playeragentmove][3] = self.fields[playeragentmove][3] + 1\n",
    "              if self.fields[playeragentmove][2] == 1 and playeragentmove >= 5 and playeragentmove <= 12:\n",
    "                self.fields[playeragentmove][2] = 0\n",
    "                self.fields[0][2] = self.fields[0][2] + 1\n",
    "                #print(\"Агент забрал шашку на\" , self.fields[playeragentmove][0])\n",
    "              #print(\"Агенту выпал\" , agentroll)\n",
    "              #print(\"Агент ходит\" , playeragentmove)\n",
    "              return playeragentmove\n",
    "            else:\n",
    "              rrr = 0#print(\"Выпал 0 и агент не делает выбора\")\n",
    "     #while True:\n",
    "     # thisbotmove = first()\n",
    "     # if thisbotmove != 4 or thisbotmove != 8 or thisbotmove != 14:\n",
    "     #    #print(\"нет второго хода бота\")\n",
    "     #    break\n",
    "\n",
    "     if self.fields[16][3] != 1:\n",
    "      thisbotmove = first()\n",
    "      #print(first())\n",
    "      if thisbotmove == 4 or thisbotmove == 8 or thisbotmove == 14:\n",
    "       #print(\"Второй ход бота\", thisbotmove)\n",
    "       self.fields[16][2] = 1\n",
    "      else:\n",
    "       self.fields[16][2] = 0\n",
    "    # if thisbotmove == 4 or thisbotmove == 8 or thisbotmove == 14:\n",
    "     if self.fields[16][2] != 1:\n",
    "      thisagentmove = second()\n",
    "      #print(second())\n",
    "      if thisagentmove == 4 or thisagentmove == 8 or thisagentmove == 14:\n",
    "        #print(\"Второй ход агента\", thisagentmove)\n",
    "        self.fields[16][3] = 1\n",
    "      else:\n",
    "        self.fields[16][3] = 0\n",
    "     #while True:\n",
    "     # thisagentmove =  second()\n",
    "     # if thisagentmove != 4 or thisagentmove != 8 or thisagentmove != 14:\n",
    "     #    #print(\"нет второго хода агента\")\n",
    "     #    break\n",
    "\n",
    "\n",
    "     terminated = bool(self.fields[15][2] >= 7 or self.fields[15][3] >= 7)\n",
    "     truncated = False  # we do not limit the number of steps here\n",
    "\n",
    "     reward = 0\n",
    "     if self.fields[15][3] >= 7 and self.fields[15][2] != 7:\n",
    "       reward = 1\n",
    "\n",
    "    # Optionally we can pass additional info, we are not using that for now\n",
    "     info = {}\n",
    "\n",
    "     self.fields = self.fields.flatten('C')\n",
    "\n",
    "     self.fields = [x/16 for x in self.fields]\n",
    "\n",
    "     #print(self.fields)\n",
    "\n",
    "     return np.array(self.fields).astype(np.float32), reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "  def render(self):\n",
    "        # agent is represented as a cross, rest as a dot\n",
    "        if self.render_mode == \"console\":\n",
    "            1==1\n",
    "\n",
    "    #print(self.fields[0])\n",
    "\n",
    "  def close(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mray\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ray\u001b[39m.\u001b[39;49minit()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/ray/_private/worker.py:1457\u001b[0m, in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, labels, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, storage, **kwargs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m         \u001b[39mreturn\u001b[39;00m RayContext(\u001b[39mdict\u001b[39m(_global_node\u001b[39m.\u001b[39maddress_info, node_id\u001b[39m=\u001b[39mnode_id\u001b[39m.\u001b[39mhex()))\n\u001b[1;32m   1456\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1458\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMaybe you called ray.init twice by accident? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1459\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThis error can be suppressed by passing in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1460\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore_reinit_error=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or by calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1461\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mray.shutdown()\u001b[39m\u001b[39m'\u001b[39m\u001b[39m prior to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mray.init()\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1462\u001b[0m         )\n\u001b[1;32m   1464\u001b[0m _system_config \u001b[39m=\u001b[39m _system_config \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m   1465\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(_system_config, \u001b[39mdict\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'."
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 22:19:14,916\tINFO util.py:159 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "# Register , how your env should be constructed (always with 5, or you can take values from the `config` EnvContext object):\n",
    "tune.register_env(\"my_env\", lambda config: GoLeftEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import gymnasium as gym\n",
    "    gymnasium = True\n",
    "except Exception:\n",
    "    import gym\n",
    "\n",
    "    gymnasium = False\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "env_name = \"my_env\"\n",
    "env = GoLeftEnv()\n",
    "#algo = PPOConfig().environment(env_name).rollouts(num_rollout_workers=1).build()\n",
    "iii = 0\n",
    "while iii<1000:\n",
    "  iii = iii+1\n",
    "  episode_reward = 0\n",
    "  terminated = truncated = False\n",
    "\n",
    "  if gymnasium:\n",
    "      obs, info = env.reset()\n",
    "  else:\n",
    "      obs = env.reset()\n",
    "\n",
    "  while not terminated and not truncated:\n",
    "      action = env.action_space.sample()\n",
    "      action = 1\n",
    "      if gymnasium:\n",
    "          obs, reward, terminated, truncated, info = env.step(action)\n",
    "      else:\n",
    "          obs, reward, terminated, info = env.step(action)\n",
    "      episode_reward += reward\n",
    "  print(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#одна тестовая игра\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    gymnasium = True\n",
    "except Exception:\n",
    "    import gym\n",
    "\n",
    "    gymnasium = False\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "env_name = \"my_env\"\n",
    "env = GoLeftEnv()\n",
    "#algo = PPOConfig().environment(env_name).rollouts(num_rollout_workers=1).build()\n",
    "iii = 0\n",
    "while iii<1:\n",
    "  iii = iii+1\n",
    "  episode_reward = 0\n",
    "  terminated = truncated = False\n",
    "\n",
    "  if gymnasium:\n",
    "      obs, info = env.reset()\n",
    "  else:\n",
    "      obs = env.reset()\n",
    "\n",
    "  while not terminated and not truncated:\n",
    "      action = env.action_space.sample()\n",
    "      #action = 1\n",
    "      #action = algo2.compute_single_action(obs)\n",
    "      if gymnasium:\n",
    "          obs, reward, terminated, truncated, info = env.step(action)\n",
    "      else:\n",
    "          obs, reward, terminated, info = env.step(action)\n",
    "      episode_reward += reward\n",
    "      print(action)\n",
    "      obs = [x if x == 0 else int(x*16) for x in obs]\n",
    "      obs = [int(a) for a in obs]\n",
    "      obs = np.array(obs)\n",
    "      obs = obs.reshape(17, 4)\n",
    "      print(obs)\n",
    "      print(reward)\n",
    "  print(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 22:19:40,623\tINFO tune.py:657 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2023-08-25 22:19:40,624\tWARNING syncer.py:260 -- You are using remote storage, but you don't have `fsspec` installed. This can lead to inefficient syncing behavior. To avoid this, install fsspec with `pip install fsspec`. Depending on your remote storage provider, consider installing the respective fsspec-package (see https://github.com/fsspec).\n",
      "2023-08-25 22:20:07,039\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "2023-08-25 22:20:07,405\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "2023-08-25 22:20:07,409\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n",
      "2023-08-25 22:20:07,455\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/gymnasium/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "2023-08-25 22:20:07,506\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-25 22:24:01</td></tr>\n",
       "<tr><td>Running for: </td><td>00:03:53.86        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.2/15.6 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 3.0/4 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8c218_00000</td><td>RUNNING </td><td>172.16.5.4:2606</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         221.192</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">    0.48</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             68.74</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 22:20:07,563\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-08-25 22:20:07,564\tWARNING deprecation.py:50 -- DeprecationWarning: `AlgorithmConfig.evaluation(evaluation_num_episodes=..)` has been deprecated. Use `AlgorithmConfig.evaluation(evaluation_duration=.., evaluation_duration_unit='episodes')` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=2606)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=2606)\u001b[0m 2023-08-25 22:20:12,365\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=2606)\u001b[0m 2023-08-25 22:20:12,365\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(PPO pid=2606)\u001b[0m 2023-08-25 22:20:12,365\tWARNING deprecation.py:50 -- DeprecationWarning: `AlgorithmConfig.evaluation(evaluation_num_episodes=..)` has been deprecated. Use `AlgorithmConfig.evaluation(evaluation_duration=.., evaluation_duration_unit='episodes')` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2675)\u001b[0m 2023-08-25 22:20:17,319\tWARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(pid=2676)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2675)\u001b[0m 2023-08-25 22:20:17,512\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2675)\u001b[0m 2023-08-25 22:20:17,512\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2675)\u001b[0m 2023-08-25 22:20:17,512\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2675)\u001b[0m 2023-08-25 22:20:17,512\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2676)\u001b[0m 2023-08-25 22:20:17,340\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=2606)\u001b[0m Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                                                                                </th><th>counters                                                                                                                    </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_sampled_throughput_per_sec</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained_throughput_per_sec</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                           </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                    </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </th><th>timers                                                                                               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_8c218_00000</td><td style=\"text-align: right;\">                  64000</td><td>{&#x27;ObsPreprocessorConnector_ms&#x27;: 0.0048182010650634766, &#x27;StateBufferConnector_ms&#x27;: 0.0033750534057617188, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.26031041145324707}</td><td>{&#x27;num_env_steps_sampled&#x27;: 64000, &#x27;num_env_steps_trained&#x27;: 0, &#x27;num_agent_steps_sampled&#x27;: 64000, &#x27;num_agent_steps_trained&#x27;: 0}</td><td>{}              </td><td style=\"text-align: right;\">             68.74</td><td>{}             </td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 0.48</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  57</td><td>{&#x27;learner&#x27;: {&#x27;__all__&#x27;: {&#x27;num_agent_steps_trained&#x27;: 128.0, &#x27;num_env_steps_trained&#x27;: 4000.0, &#x27;total_loss&#x27;: 0.11531132242497383}, &#x27;default_policy&#x27;: {&#x27;total_loss&#x27;: 0.11531132242497383, &#x27;policy_loss&#x27;: -0.004669851620496908, &#x27;vf_loss&#x27;: 0.11864918437816187, &#x27;vf_loss_unclipped&#x27;: 0.11864918437816187, &#x27;vf_explained_var&#x27;: 0.04657289284124557, &#x27;entropy&#x27;: 1.6665341601188757, &#x27;mean_kl_loss&#x27;: 0.005919951064206805, &#x27;curr_lr&#x27;: 5e-05, &#x27;curr_entropy_coeff&#x27;: 0.0, &#x27;curr_kl_coeff&#x27;: 0.22500000894069672}}, &#x27;num_env_steps_sampled&#x27;: 64000, &#x27;num_env_steps_trained&#x27;: 0, &#x27;num_agent_steps_sampled&#x27;: 64000, &#x27;num_agent_steps_trained&#x27;: 0}</td><td style=\"text-align: right;\">                    64000</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">                  64000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                                   319.341</td><td style=\"text-align: right;\">                      0</td><td style=\"text-align: right;\">                                0</td><td style=\"text-align: right;\">                                         0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    2</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                            0</td><td>{&#x27;cpu_util_percent&#x27;: 38.37222222222222, &#x27;ram_util_percent&#x27;: 26.927777777777774}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.5149210451923741, &#x27;mean_inference_ms&#x27;: 1.4596046309567856, &#x27;mean_action_processing_ms&#x27;: 0.11771713486815756, &#x27;mean_env_wait_ms&#x27;: 0.1416960313486475, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 1.0, &#x27;episode_reward_min&#x27;: 0.0, &#x27;episode_reward_mean&#x27;: 0.48, &#x27;episode_len_mean&#x27;: 68.74, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 57, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], &#x27;episode_lengths&#x27;: [64, 69, 72, 73, 72, 62, 71, 61, 74, 69, 62, 64, 66, 61, 72, 61, 61, 77, 70, 74, 55, 60, 73, 66, 73, 78, 73, 70, 69, 62, 61, 68, 69, 73, 65, 61, 69, 67, 60, 80, 71, 60, 66, 64, 63, 64, 71, 66, 78, 59, 60, 54, 69, 74, 66, 67, 71, 72, 66, 54, 76, 77, 71, 77, 63, 71, 82, 64, 75, 70, 70, 74, 77, 84, 65, 76, 66, 60, 81, 76, 72, 74, 68, 67, 58, 68, 76, 66, 72, 70, 67, 72, 66, 56, 73, 78, 80, 66, 74, 74]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.5149210451923741, &#x27;mean_inference_ms&#x27;: 1.4596046309567856, &#x27;mean_action_processing_ms&#x27;: 0.11771713486815756, &#x27;mean_env_wait_ms&#x27;: 0.1416960313486475, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.0048182010650634766, &#x27;StateBufferConnector_ms&#x27;: 0.0033750534057617188, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.26031041145324707}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 13371.129, &#x27;sample_time_ms&#x27;: 4593.727, &#x27;synch_weights_time_ms&#x27;: 4.259}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 22:24:01,380\tWARNING tune.py:192 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2023-08-25 22:24:11,427\tINFO tune.py:1148 -- Total run time: 270.80 seconds (233.82 seconds for the tuning loop).\n",
      "2023-08-25 22:24:11,431\tWARNING tune.py:1163 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f31b02d0f40>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "tune.run(\"PPO\",\n",
    "         config={\"env\": \"my_env\",\n",
    "                 \"evaluation_interval\": 10,\n",
    "                 \"evaluation_num_episodes\": 50,\n",
    "                 \"num_workers\": 2,\n",
    "                 # other configurations go here, if none provided, then default configurations will be used\n",
    "                 },\n",
    "         storage_path=\"Urrnow70\",\n",
    "         checkpoint_freq=10\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 22:24:20,812\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-08-25 22:24:20,814\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "2023-08-25 22:24:20,816\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "\u001b[2m\u001b[36m(pid=4515)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4515)\u001b[0m 2023-08-25 22:24:25,026\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4514)\u001b[0m 2023-08-25 22:24:24,988\tWARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4514)\u001b[0m 2023-08-25 22:24:25,023\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4514)\u001b[0m 2023-08-25 22:24:25,023\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4514)\u001b[0m 2023-08-25 22:24:25,023\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4514)\u001b[0m 2023-08-25 22:24:25,023\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "2023-08-25 22:24:25,079\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-08-25 22:24:25,087\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "2023-08-25 22:24:25,088\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "2023-08-25 22:24:25,089\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "2023-08-25 22:24:25,089\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "2023-08-25 22:24:25,105\tWARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "2023-08-25 22:24:25,108\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-08-25 22:24:25,122\tINFO util.py:159 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-08-25 22:24:25,171\tWARNING util.py:68 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "algo2 = Algorithm.from_checkpoint(\"/workspaces/codespaces-jupyter/notebooks/Urrnow70/PPO/PPO_my_env_8c218_00000_0_2023-08-25_22-20-07/checkpoint_000010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import gymnasium as gym\n",
    "    gymnasium = True\n",
    "except Exception:\n",
    "    import gym\n",
    "\n",
    "    gymnasium = False\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "env_name = \"my_env\"\n",
    "env = GoLeftEnv()\n",
    "#algo = PPOConfig().environment(env_name).rollouts(num_rollout_workers=1).build()\n",
    "iii = 0\n",
    "while iii<100:\n",
    "  iii = iii+1\n",
    "  episode_reward = 0\n",
    "  terminated = truncated = False\n",
    "\n",
    "  if gymnasium:\n",
    "      obs, info = env.reset()\n",
    "  else:\n",
    "      obs = env.reset()\n",
    "\n",
    "  while not terminated and not truncated:\n",
    "      action = algo2.compute_single_action(obs)\n",
    "      if gymnasium:\n",
    "          obs, reward, terminated, truncated, info = env.step(action)\n",
    "      else:\n",
    "          obs, reward, terminated, info = env.step(action)\n",
    "      episode_reward += reward\n",
    "  print(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#одна тестовая игра\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    gymnasium = True\n",
    "except Exception:\n",
    "    import gym\n",
    "\n",
    "    gymnasium = False\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "env_name = \"my_env\"\n",
    "env = GoLeftEnv()\n",
    "#algo = PPOConfig().environment(env_name).rollouts(num_rollout_workers=1).build()\n",
    "iii = 0\n",
    "while iii<1:\n",
    "  iii = iii+1\n",
    "  episode_reward = 0\n",
    "  terminated = truncated = False\n",
    "\n",
    "  if gymnasium:\n",
    "      obs, info = env.reset()\n",
    "  else:\n",
    "      obs = env.reset()\n",
    "\n",
    "  while not terminated and not truncated:\n",
    "      action = algo2.compute_single_action(obs)\n",
    "      #action = 1\n",
    "      #action = algo2.compute_single_action(obs)\n",
    "      if gymnasium:\n",
    "          obs, reward, terminated, truncated, info = env.step(action)\n",
    "      else:\n",
    "          obs, reward, terminated, info = env.step(action)\n",
    "      episode_reward += reward\n",
    "      print(action)\n",
    "      obs = [x if x == 0 else int(x*16) for x in obs]\n",
    "      obs = [int(a) for a in obs]\n",
    "      obs = np.array(obs)\n",
    "      obs = obs.reshape(17, 4)\n",
    "      print(obs)\n",
    "      print(reward)\n",
    "      obs = obs.flatten('C')\n",
    "      sobs = [x/16 for x in obs]\n",
    "  print(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#одна тестовая игра с рендерингом\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    gymnasium = True\n",
    "except Exception:\n",
    "    import gym\n",
    "\n",
    "    gymnasium = False\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "env_name = \"my_env\"\n",
    "env = GoLeftEnv()\n",
    "#algo = PPOConfig().environment(env_name).rollouts(num_rollout_workers=1).build()\n",
    "iii = 0\n",
    "while iii<1:\n",
    "  iii = iii+1\n",
    "  episode_reward = 0\n",
    "  terminated = truncated = False\n",
    "\n",
    "  if gymnasium:\n",
    "      obs, info = env.reset()\n",
    "  else:\n",
    "      obs = env.reset()\n",
    "\n",
    "  while not terminated and not truncated:\n",
    "      action = algo2.compute_single_action(obs)\n",
    "      #action = 1\n",
    "      #action = algo2.compute_single_action(obs)\n",
    "      if gymnasium:\n",
    "          obs, reward, terminated, truncated, info = env.step(action)\n",
    "      else:\n",
    "          obs, reward, terminated, info = env.step(action)\n",
    "      episode_reward += reward\n",
    "      print(action)\n",
    "      obs = [x if x == 0 else int(x*16) for x in obs]\n",
    "      obs = [int(a) for a in obs]\n",
    "      obs = np.array(obs)\n",
    "      obs = obs.reshape(17, 4)\n",
    "      print(obs)\n",
    "      print(reward)\n",
    "      if obs[4][2] == 1:\n",
    "       print(u\"\\U0001F7E5\", end=\"\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\", end=\"\")\n",
    "      if obs[5][2] == 1:\n",
    "       print(u\"\\U0001F7E5\", end=\"\")\n",
    "      elif obs[5][3] == 1:\n",
    "       print(u\"\\U0001F7E6\", end=\"\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\", end=\"\")\n",
    "      if obs[4][3] == 1:\n",
    "       print(u\"\\U0001F7E6\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\")\n",
    "      if obs[3][2] == 1:\n",
    "       print(u\"\\U0001F7E5\", end=\"\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\", end=\"\")\n",
    "      if obs[6][2] == 1:\n",
    "       print(u\"\\U0001F7E5\", end=\"\")\n",
    "      elif obs[6][3] == 1:\n",
    "       print(u\"\\U0001F7E6\", end=\"\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\", end=\"\")\n",
    "      if obs[3][3] == 1:\n",
    "       print(u\"\\U0001F7E6\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\")\n",
    "      if obs[2][2] == 1:\n",
    "       print(u\"\\U0001F7E5\", end=\"\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\", end=\"\")\n",
    "      if obs[7][2] == 1:\n",
    "       print(u\"\\U0001F7E5\", end=\"\")\n",
    "      elif obs[7][3] == 1:\n",
    "       print(u\"\\U0001F7E6\", end=\"\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\", end=\"\")\n",
    "      if obs[2][3] == 1:\n",
    "       print(u\"\\U0001F7E6\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\")\n",
    "      if obs[1][2] == 1:\n",
    "       print(u\"\\U0001F7E5\", end=\"\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\", end=\"\")\n",
    "      if obs[8][2] == 1:\n",
    "       print(u\"\\U0001F7E5\", end=\"\")\n",
    "      elif obs[8][3] == 1:\n",
    "       print(u\"\\U0001F7E6\", end=\"\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\", end=\"\")\n",
    "      if obs[1][3] == 1:\n",
    "       print(u\"\\U0001F7E6\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\")\n",
    "      print(u\"\\u2B1B\", end=\"\")\n",
    "      if obs[9][2] == 1:\n",
    "       print(u\"\\U0001F7E5\", end=\"\")\n",
    "      elif obs[9][3] == 1:\n",
    "       print(u\"\\U0001F7E6\", end=\"\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\", end=\"\")\n",
    "      print(u\"\\u2B1B\")\n",
    "      print(u\"\\u2B1B\", end=\"\")\n",
    "      if obs[10][2] == 1:\n",
    "       print(u\"\\U0001F7E5\", end=\"\")\n",
    "      elif obs[10][3] == 1:\n",
    "       print(u\"\\U0001F7E6\", end=\"\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\", end=\"\")\n",
    "      print(u\"\\u2B1B\")\n",
    "      if obs[14][2] == 1:\n",
    "       print(u\"\\U0001F7E5\", end=\"\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\", end=\"\")\n",
    "      if obs[11][2] == 1:\n",
    "       print(u\"\\U0001F7E5\", end=\"\")\n",
    "      elif obs[11][3] == 1:\n",
    "       print(u\"\\U0001F7E6\", end=\"\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\", end=\"\")\n",
    "      if obs[14][3] == 1:\n",
    "       print(u\"\\U0001F7E6\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\")\n",
    "      if obs[13][2] == 1:\n",
    "       print(u\"\\U0001F7E5\", end=\"\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\", end=\"\")\n",
    "      if obs[12][2] == 1:\n",
    "       print(u\"\\U0001F7E5\", end=\"\")\n",
    "      elif obs[12][3] == 1:\n",
    "       print(u\"\\U0001F7E6\", end=\"\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\", end=\"\")\n",
    "      if obs[13][3] == 1:\n",
    "       print(u\"\\U0001F7E6\")\n",
    "      else:\n",
    "       print(u\"\\u2B1C\")  \n",
    "      obs = obs.flatten('C')\n",
    "      sobs = [x/16 for x in obs]\n",
    "  print(episode_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
