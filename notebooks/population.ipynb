{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population Data from CSV\n",
    "\n",
    "This notebooks reads sample population data from `data/atlantis.csv` and plots it using Matplotlib. Edit `data/atlantis.csv` and re-run this cell to see how the plots change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"ray[rllib]\" tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"ray[rllib]\" torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.dreamerv3 import DreamerV3Config\n",
    "config = DreamerV3Config()\n",
    "algo = config.build(env=\"CartPole-v1\")  # doctest: +SKIP\n",
    "algo.train()  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "# Configure.\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "config = PPOConfig().environment(env=\"CartPole-v1\").training(train_batch_size=4000)\n",
    "\n",
    "# Train via Ray Tune.\n",
    "tune.run(\"PPO\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure.\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "config = PPOConfig().environment(env=\"CartPole-v1\").training(train_batch_size=4000)\n",
    "\n",
    "# Build.\n",
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "while True:\n",
    "    print(algo.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.rllib.algorithms.dreamerv3.dreamerv3 import DreamerV3Config\n",
    "\n",
    "\n",
    "def _env_creator(ctx):\n",
    "    import flappy_bird_gymnasium  # doctest: +SKIP\n",
    "    import gymnasium as gym\n",
    "    from supersuit.generic_wrappers import resize_v1\n",
    "    from ray.rllib.algorithms.dreamerv3.utils.env_runner import NormalizedImageEnv\n",
    "\n",
    "    return NormalizedImageEnv(\n",
    "        resize_v1(  # resize to 64x64 and normalize images\n",
    "            gym.make(\"FlappyBird-rgb-v0\", audio_on=False), x_size=64, y_size=64\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Register the FlappyBird-rgb-v0 env including necessary wrappers via the\n",
    "# `tune.register_env()` API.\n",
    "tune.register_env(\"flappy-bird\", _env_creator)\n",
    "\n",
    "# Define the `config` variable to use for training.\n",
    "config = (\n",
    "    DreamerV3Config()\n",
    "    # set the env to the pre-registered string\n",
    "    .environment(\"flappy-bird\")\n",
    "    # play around with the insanely high number of hyperparameters for DreamerV3 ;) \n",
    "    .training(\n",
    "        model_size=\"S\",\n",
    "        training_ratio=1024,\n",
    "    )\n",
    ")\n",
    "#dasgdashadhadh\n",
    "# Run the tuner job.\n",
    "results = tune.Tuner(trainable=\"DreamerV3\", param_space=config).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "#мы заменили float и np.array\n",
    "class GoLeftEnv(gym.Env):\n",
    "  \"\"\"\n",
    "  Custom Environment that follows gym interface.\n",
    "  This is a simple env where the agent must learn to go always left.\n",
    "  \"\"\"\n",
    "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "  metadata = {'render.modes': ['console']}\n",
    "  # Define constants for clearer code\n",
    "  FIRST = 0\n",
    "  SECOND = 1\n",
    "  THIRD = 2\n",
    "  FORTH = 3\n",
    "  FIFTH= 4\n",
    "  SIXTH = 5\n",
    "  SEVENTH = 6\n",
    "\n",
    "  def __init__(self, grid_size=10, render_mode=\"console\"):\n",
    "    super(GoLeftEnv, self).__init__()\n",
    "    self.render_mode = render_mode\n",
    "\n",
    "    self.fields = np.array([[0, 0, 7, 7,], [1, 0, 0, 0,], [2, 0, 0, 0,], [3, 0, 0, 0,], [4, 1, 0, 0,], [5, 0, 0, 0], [6, 0, 0, 0], [7, 0, 0, 0], [8, 1, 0, 0], [9, 0, 0, 0], [10, 0, 0, 0], [11, 0, 0, 0], [12, 0, 0, 0],\n",
    "                   [13, 0, 0, 0], [14, 1, 0, 0], [15, 0, 0, 0], [0, 0, 0, 0]]).flatten('C')\n",
    "\n",
    "    # Define action and observation space\n",
    "    # They must be gym.spaces objects\n",
    "    # Example when using discrete actions, we have two: left and right\n",
    "    n_actions = 7\n",
    "    self.action_space = spaces.Discrete(n_actions)\n",
    "    # The observation will be the coordinate of the agent\n",
    "    # this can be described both by Discrete and Box space\n",
    "    self.observation_space = spaces.Box(low=-16, high=16,\n",
    "                                        shape=(68,), dtype=np.float32)\n",
    "  #попробовать переделать всё в np.array\n",
    "\n",
    "  def reset(self, seed=None, options=None):\n",
    "    \"\"\"\n",
    "    Important: the observation must be a numpy array\n",
    "    :return: (np.array)\n",
    "    \"\"\"\n",
    "    super().reset(seed=seed, options=options)\n",
    "\n",
    "\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.fields = np.array([[0, 0, 7, 7,], [1, 0, 0, 0,], [2, 0, 0, 0,], [3, 0, 0, 0,], [4, 1, 0, 0,], [5, 0, 0, 0], [6, 0, 0, 0], [7, 0, 0, 0], [8, 1, 0, 0], [9, 0, 0, 0], [10, 0, 0, 0], [11, 0, 0, 0], [12, 0, 0, 0],\n",
    "                   [13, 0, 0, 0], [14, 1, 0, 0], [15, 0, 0, 0], [0, 0, 0, 0]])\n",
    "\n",
    "\n",
    "\n",
    "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "    return self.fields.astype(np.float32).flatten('C'), {}\n",
    "\n",
    "  def step(self, action):\n",
    "\n",
    "     self.fields = self.fields.reshape(17, 4)\n",
    "\n",
    "     def first():\n",
    "            numberforcycles = 15\n",
    "\n",
    "            balcycle1 = 0\n",
    "\n",
    "            roll = self.fields[16][0]\n",
    "\n",
    "            lst = [0, 1, 2, 3, 4]\n",
    "            weights = [6.25, 25, 37.5, 25, 6.25]\n",
    "            nextroll = random.choices(lst, weights=weights, k=1)\n",
    "            nextroll = nextroll[0]\n",
    "            self.fields[16][0] = nextroll\n",
    "            #print(\"Выпала\" , roll)\n",
    "            arrplayer1 = []\n",
    "            while balcycle1 < numberforcycles:\n",
    "              ballnow = self.fields[balcycle1][2]\n",
    "              ballindex = self.fields[balcycle1][0]\n",
    "          #расположение фишек игрока 1\n",
    "              if ballnow != 0:\n",
    "               arrplayer1.append(ballindex)\n",
    "              balcycle1 += 1\n",
    "            if self.fields[8][3] == 1:\n",
    "                arrplayer1.append(8)\n",
    "            #print(arrplayer1)\n",
    "\n",
    "            balcycle1 = 0\n",
    "            arrchoice1 = []\n",
    "            while balcycle1 < numberforcycles:\n",
    "              ballnow = self.fields[balcycle1][2]\n",
    "              ballindex = self.fields[balcycle1][0]\n",
    "              if ballnow != 0:\n",
    "                ballfuture = ballindex + roll\n",
    "                if ballfuture <= numberforcycles:\n",
    "                 if ballfuture not in arrplayer1:\n",
    "                  arrchoice1.append(ballfuture)\n",
    "              # print(\"Выпала\" , self.roll)\n",
    "                  #print(\"текущая позиция\" , ballindex)\n",
    "                  #print(\"возможная позиция\" , ballfuture)\n",
    "              balcycle1 += 1\n",
    "\n",
    "\n",
    "\n",
    "            if arrchoice1:\n",
    "\n",
    "              #случайный бот:\n",
    "              playeronemove = random.choice(arrchoice1)\n",
    "\n",
    "              #Очень жадный бот:\n",
    "              #playeronemove = len(arrchoice1)\n",
    "              #playeronemove = arrchoice1[playeronemove-1]\n",
    "\n",
    "              #Жадный бот:\n",
    "              #playeronemove = 33\n",
    "              #for arrchoice in arrchoice1:\n",
    "              #  if self.fields[arrchoice][1] == 1 or (self.fields[arrchoice][3] == 1 and self.fields[arrchoice][0] >= 5 and self.fields[arrchoice][0] <= 12):\n",
    "              #    playeronemove = self.fields[arrchoice][0]\n",
    "              #if playeronemove == 33:\n",
    "              #  playeronemove = len(arrchoice1)\n",
    "              #  playeronemove = arrchoice1[playeronemove-1]\n",
    "\n",
    "              #playeronemove = random.choice(arrchoice1)\n",
    "              playeronepos = playeronemove - roll\n",
    "              #print(playeronepos)\n",
    "              self.fields[playeronepos][2] = self.fields[playeronepos][2] - 1\n",
    "              self.fields[playeronemove][2] = self.fields[playeronemove][2] + 1\n",
    "              if self.fields[playeronemove][3] == 1 and playeronemove >= 5 and playeronemove <= 12:\n",
    "                self.fields[playeronemove][3] = 0\n",
    "                self.fields[0][3] = self.fields[0][3] + 1\n",
    "                #print(\"мы забрали шашку на\" , self.fields[playeronemove][0])\n",
    "              #print(\"боту выпал\", roll)\n",
    "              #print(\"бот ходит\", playeronemove)\n",
    "              return playeronemove\n",
    "\n",
    "     def second():\n",
    "            numberforcycles = 15\n",
    "\n",
    "            agentroll = self.fields[16][1]\n",
    "\n",
    "\n",
    "            lst = [0, 1, 2, 3, 4]\n",
    "            weights = [6.25, 25, 37.5, 25, 6.25]\n",
    "            nextagentroll = random.choices(lst, weights=weights, k=1)\n",
    "            nextagentroll = nextagentroll[0]\n",
    "            self.fields[16][1] = nextagentroll\n",
    "            #print(\"Агенту Выпал\" , agentroll)\n",
    "\n",
    "            agentcycle = 0\n",
    "            arrplayeragent = []\n",
    "\n",
    "            while agentcycle < numberforcycles:\n",
    "              agentballnow = self.fields[agentcycle][3]\n",
    "              agentballindex = self.fields[agentcycle][0]\n",
    "          #расположение фишек игрока 2\n",
    "              if agentballnow != 0:\n",
    "               arrplayeragent.append(agentballindex)\n",
    "              agentcycle += 1\n",
    "            if self.fields[8][2] == 1:\n",
    "                arrplayeragent.append(8)\n",
    "\n",
    "            #print(\"фишки агента\", arrplayeragent)\n",
    "\n",
    "            agentcycle = 0\n",
    "            arrplayeragent2 = []\n",
    "\n",
    "            while agentcycle < numberforcycles:\n",
    "              agentballnow = self.fields[agentcycle][3]\n",
    "              agentballindex = self.fields[agentcycle][0]\n",
    "          #будущее фишек игрока 2\n",
    "              if agentballnow != 0:\n",
    "               agentballfuture = agentballindex + agentroll\n",
    "               if agentballfuture <= numberforcycles:\n",
    "                if agentballfuture not in arrplayeragent:\n",
    "                  arrplayeragent2.append(agentballfuture)\n",
    "              agentcycle += 1\n",
    "\n",
    "            #print(arrplayeragent2)\n",
    "\n",
    "\n",
    "\n",
    "            if arrplayeragent2:\n",
    "              playeragentmove = len(arrplayeragent2)\n",
    "              playeragentmove = arrplayeragent2[0]#arrplayeragent2[playeragentmove-1]\n",
    "\n",
    "              if action == self.FIRST:\n",
    "                if len(arrplayeragent2) > 0:\n",
    "                 playeragentmove = arrplayeragent2[0]\n",
    "                else:\n",
    "                 playeragentmove = playeragentmove\n",
    "              elif action == self.SECOND:\n",
    "               if len(arrplayeragent2) > 1:\n",
    "                playeragentmove = arrplayeragent2[1]\n",
    "               else:\n",
    "                playeragentmove = playeragentmove\n",
    "              elif action == self.THIRD:\n",
    "               if len(arrplayeragent2) > 2:\n",
    "                playeragentmove = arrplayeragent2[2]\n",
    "               else:\n",
    "                playeragentmove = playeragentmove\n",
    "              elif action == self.FORTH:\n",
    "               if len(arrplayeragent2) > 3:\n",
    "                playeragentmove = arrplayeragent2[3]\n",
    "               else:\n",
    "                playeragentmove = playeragentmove\n",
    "              elif action == self.FIFTH:\n",
    "               if len(arrplayeragent2) > 4:\n",
    "                playeragentmove = arrplayeragent2[4]\n",
    "               else:\n",
    "                playeragentmove = playeragentmove\n",
    "              elif action == self.SIXTH:\n",
    "               if len(arrplayeragent2) > 5:\n",
    "                playeragentmove = arrplayeragent2[5]\n",
    "               else:\n",
    "                playeragentmove = playeragentmove\n",
    "              elif action == self.SEVENTH:\n",
    "               if len(arrplayeragent2) > 6:\n",
    "                playeragentmove = arrplayeragent2[6]\n",
    "               else:\n",
    "                playeragentmove = playeragentmove\n",
    "              else:\n",
    "                raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "\n",
    "\n",
    "\n",
    "              #print(\"Агент делает выбор\", playeragentmove)\n",
    "\n",
    "              #playeragentmove = random.choice(arrplayeragent)\n",
    "              playeragentpos = playeragentmove - agentroll\n",
    "              self.fields[playeragentpos][3] = self.fields[playeragentpos][3] - 1\n",
    "              self.fields[playeragentmove][3] = self.fields[playeragentmove][3] + 1\n",
    "              if self.fields[playeragentmove][2] == 1 and playeragentmove >= 5 and playeragentmove <= 12:\n",
    "                self.fields[playeragentmove][2] = 0\n",
    "                self.fields[0][2] = self.fields[0][2] + 1\n",
    "                #print(\"Агент забрал шашку на\" , self.fields[playeragentmove][0])\n",
    "              #print(\"Агенту выпал\" , agentroll)\n",
    "              #print(\"Агент ходит\" , playeragentmove)\n",
    "              return playeragentmove\n",
    "            else:\n",
    "              rrr = 0#print(\"Выпал 0 и агент не делает выбора\")\n",
    "     #while True:\n",
    "     # thisbotmove = first()\n",
    "     # if thisbotmove != 4 or thisbotmove != 8 or thisbotmove != 14:\n",
    "     #    #print(\"нет второго хода бота\")\n",
    "     #    break\n",
    "\n",
    "     if self.fields[16][3] != 1:\n",
    "      thisbotmove = first()\n",
    "      #print(first())\n",
    "      if thisbotmove == 4 or thisbotmove == 8 or thisbotmove == 14:\n",
    "       #print(\"Второй ход бота\", thisbotmove)\n",
    "       self.fields[16][2] = 1\n",
    "      else:\n",
    "       self.fields[16][2] = 0\n",
    "    # if thisbotmove == 4 or thisbotmove == 8 or thisbotmove == 14:\n",
    "     if self.fields[16][2] != 1:\n",
    "      thisagentmove = second()\n",
    "      #print(second())\n",
    "      if thisagentmove == 4 or thisagentmove == 8 or thisagentmove == 14:\n",
    "        #print(\"Второй ход агента\", thisagentmove)\n",
    "        self.fields[16][3] = 1\n",
    "      else:\n",
    "        self.fields[16][3] = 0\n",
    "     #while True:\n",
    "     # thisagentmove =  second()\n",
    "     # if thisagentmove != 4 or thisagentmove != 8 or thisagentmove != 14:\n",
    "     #    #print(\"нет второго хода агента\")\n",
    "     #    break\n",
    "\n",
    "\n",
    "     terminated = bool(self.fields[15][2] >= 7 or self.fields[15][3] >= 7)\n",
    "     truncated = False  # we do not limit the number of steps here\n",
    "\n",
    "     reward = 0\n",
    "     if self.fields[15][3] >= 7 and self.fields[15][2] != 7:\n",
    "       reward = 1\n",
    "\n",
    "    # Optionally we can pass additional info, we are not using that for now\n",
    "     info = {}\n",
    "\n",
    "     return np.array(self.fields).astype(np.float32).flatten('C'), reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "  def render(self):\n",
    "        # agent is represented as a cross, rest as a dot\n",
    "        if self.render_mode == \"console\":\n",
    "            1==1\n",
    "\n",
    "    #print(self.fields[0])\n",
    "\n",
    "  def close(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.alpha_zero import AlphaZeroConfig\n",
    "config = AlphaZeroConfig()   \n",
    "config = config.training(sgd_minibatch_size=256)   \n",
    "config = config.resources(num_gpus=0)   \n",
    "config = config.rollouts(num_rollout_workers=2)   \n",
    "print(config.to_dict()) \n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build(env=\"CartPole-v1\")  \n",
    "algo.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "# Register , how your env should be constructed (always with 5, or you can take values from the `config` EnvContext object):\n",
    "tune.register_env(\"my_env\", lambda config: GoLeftEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "class GoLeftEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that follows gym interface.\n",
    "    This is a simple env where the agent must learn to go always left.\n",
    "    \"\"\"\n",
    "\n",
    "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "    metadata = {\"render_modes\": [\"console\"]}\n",
    "\n",
    "    # Define constants for clearer code\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "\n",
    "    def __init__(self, grid_size=10, render_mode=\"console\"):\n",
    "        super(GoLeftEnv, self).__init__()\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Size of the 1D-grid\n",
    "        self.grid_size = grid_size\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_pos = grid_size - 1\n",
    "\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions, we have two: left and right\n",
    "        n_actions = 2\n",
    "        self.action_space = spaces.Discrete(n_actions)\n",
    "        # The observation will be the coordinate of the agent\n",
    "        # this can be described both by Discrete and Box space\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=self.grid_size, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array)\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed, options=options)\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_pos = self.grid_size - 1\n",
    "        # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "        return np.array([self.agent_pos]).astype(np.float32), {}  # empty info dict\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.LEFT:\n",
    "            self.agent_pos -= 1\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos += 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Received invalid action={action} which is not part of the action space\"\n",
    "            )\n",
    "\n",
    "        # Account for the boundaries of the grid\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "        # Are we at the left of the grid?\n",
    "        terminated = bool(self.agent_pos == 0)\n",
    "        truncated = False  # we do not limit the number of steps here\n",
    "\n",
    "        # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "        reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "        # Optionally we can pass additional info, we are not using that for now\n",
    "        info = {}\n",
    "\n",
    "        return (\n",
    "            np.array([self.agent_pos]).astype(np.float32),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        # agent is represented as a cross, rest as a dot\n",
    "        if self.render_mode == \"console\":\n",
    "            print(\".\" * self.agent_pos, end=\"\")\n",
    "            print(\"x\", end=\"\")\n",
    "            print(\".\" * (self.grid_size - self.agent_pos))\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.alpha_zero import AlphaZeroConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = AlphaZeroConfig()\n",
    "# Print out some default values.\n",
    "print(config.shuffle_sequences) \n",
    "# Update the config object.\n",
    "config.training(lr=tune.grid_search([0.001, 0.0001]))  \n",
    "# Set the config object's env.\n",
    "config.environment(env=\"CartPole-v1\")   \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner( \n",
    "    \"AlphaZero\",\n",
    "    run_config=air.RunConfig(stop={\"episode_reward_mean\": 200}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.apex_dqn.apex_dqn import ApexDQNConfig\n",
    "config = ApexDQNConfig()\n",
    "print(config.replay_buffer_config) \n",
    "replay_config = config.replay_buffer_config.update( \n",
    "    {\n",
    "        \"capacity\": 100000,\n",
    "        \"prioritized_replay_alpha\": 0.45,\n",
    "        \"prioritized_replay_beta\": 0.55,\n",
    "        \"prioritized_replay_eps\": 3e-6,\n",
    "    }\n",
    ")\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=4)  \n",
    "config = config.environment(\"CartPole-v1\")  \n",
    "algo = config.build() \n",
    "for _ in range(5):\n",
    "    print(algo.train())  # 3. train it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.appo import APPOConfig\n",
    "config = APPOConfig().training(lr=0.01, grad_clip=30.0)\n",
    "config = config.resources(num_gpus=0)\n",
    "config = config.rollouts(num_rollout_workers=2)\n",
    "config = config.environment(\"my_env\")\n",
    "print(config.to_dict())  \n",
    "# Build an Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build()  \n",
    "\n",
    "for _ in range(5):\n",
    "    print(algo.train())  # 3. train it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "         from ray.rllib.algorithms.dreamer import DreamerConfig\n",
    "         config = DreamerConfig().training(gamma=0.9, lr=0.01)  # doctest: +SKIP\n",
    "         config = config.resources(num_gpus=0)  # doctest: +SKIP\n",
    "         config = config.rollouts(num_rollout_workers=0)  # doctest: +SKIP\n",
    "         print(config.to_dict())  # doctest: +SKIP\n",
    "         # Build a Algorithm object from the config and run 1 training iteration.\n",
    "         algo = config.build(env=\"Taxi-v3\")  # doctest: +SKIP\n",
    "         algo.train()  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "         from ray.rllib.algorithms.dreamerv3 import DreamerV3Config\n",
    "         config = DreamerV3Config()\n",
    "         config = config.training(  # doctest: +SKIP\n",
    "             batch_size_B=8, model_size=\"M\"\n",
    "         )\n",
    "         config = config.resources(num_learner_workers=4)  # doctest: +SKIP\n",
    "         print(config.to_dict())  # doctest: +SKIP\n",
    "         # Build a Algorithm object from the config and run 1 training iteration.\n",
    "         algo = config.build(env=\"CartPole-v1\")  # doctest: +SKIP\n",
    "         algo.train()  # doctest: +SKIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.dreamerv3.dreamerv3 import DreamerV3Config\n",
    "\n",
    "# Run with:\n",
    "# python run_regression_tests.py --dir [this file]\n",
    "\n",
    "config = (\n",
    "    DreamerV3Config()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .training(\n",
    "        model_size=\"XS\",\n",
    "        training_ratio=1024,\n",
    "    )\n",
    ")\n",
    "\n",
    "tune.Tuner(trainable=\"DreamerV3\", param_space=config).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import air\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.dreamer import DreamerConfig\n",
    "config = DreamerConfig()\n",
    "# Print out some default values.\n",
    "# Update the config object.\n",
    "config = config.training(  \n",
    "    lr=tune.grid_search([0.001, 0.0001]))\n",
    "# Set the config object's env.\n",
    "config = config.environment(env=\"CartPole-v1\")  \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"Dreamer\",\n",
    "    run_config=air.RunConfig(stop={\"episode_reward_mean\": 200}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.dreamer import DreamerConfig\n",
    "config = DreamerConfig().training(gamma=0.9, lr=0.01)  \n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=0)  \n",
    "print(config.to_dict())  \n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build(env=\"CartPole-v1\")  \n",
    "algo.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.dreamer import DreamerConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "\n",
    "algo = (\n",
    "    DreamerConfig()\n",
    "    .rollouts(num_rollout_workers=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"CartPole-v1\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "    print(pretty_print(result))\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GoLeftEnv()\n",
    "\n",
    "for _ in range(100):\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        action = np.array(1)\n",
    "        obs, r, done, _, _ = env.step(action)\n",
    "        print(obs)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.dreamerv3.dreamerv3 import DreamerV3Config\n",
    "\n",
    "config = (\n",
    "    DreamerV3Config()\n",
    "    # set the env to the pre-registered string\n",
    "    .environment(\"my_env\")\n",
    "    # play around with the insanely high number of hyperparameters for DreamerV3 ;) \n",
    "    .training(\n",
    "        model_size=\"S\",\n",
    "        training_ratio=1024,\n",
    "    )\n",
    ")\n",
    "\n",
    "results = tune.Tuner(trainable=\"DreamerV3\", param_space=config).fit()\n",
    "\n",
    "print(777777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "# Configure.\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "config = PPOConfig().environment(env=\"my_env\").training(train_batch_size=4000)\n",
    "\n",
    "# Train via Ray Tune.\n",
    "tune.run(\"PPO\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure.\n",
    "from ray.rllib.algorithms.dreamerv3.dreamerv3 import DreamerV3Config\n",
    "config = DreamerV3Config().environment(env=\"CartPole-v1\")\n",
    "\n",
    "# Build.\n",
    "algo = config.build()\n",
    "\n",
    "# Train.\n",
    "while True:\n",
    "    print(algo.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.dreamerv3.dreamerv3 import DreamerV3Config\n",
    "\n",
    "config = (  # 1. Configure the algorithm,\n",
    "    DreamerV3Config()\n",
    "    .environment(\"MountainCar-v0\")\n",
    "    .framework(\"tf2\")\n",
    "    .training(model_size=\"XS\",training_ratio=1024,)\n",
    ")\n",
    "\n",
    "algo = config.build()  # 2. build the algorithm,\n",
    "\n",
    "for _ in range(5):\n",
    "    print(algo.train())  # 3. train it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.dreamerv3.dreamerv3 import DreamerV3Config\n",
    "\n",
    "config = (  # 1. Configure the algorithm,\n",
    "    DreamerV3Config()\n",
    "    .environment(\"ALE/Alien-v5\")\n",
    "    .rollouts(num_rollout_workers=1)\n",
    ")\n",
    "\n",
    "algo = config.build()  # 2. build the algorithm,\n",
    "\n",
    "for _ in range(5):\n",
    "    print(algo.train())  # 3. train it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "tune.run(\"PPO\",\n",
    "         config={\"env\": \"my_env\",\n",
    "                 \"evaluation_interval\": 10,\n",
    "                 \"evaluation_num_episodes\": 50,\n",
    "                 \"num_workers\": 1,\n",
    "                 # other configurations go here, if none provided, then default configurations will be used\n",
    "                 },\n",
    "         storage_path=\"Urrnow4\",\n",
    "         checkpoint_freq=10\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "algo2 = Algorithm.from_checkpoint(\"/workspaces/codespaces-jupyter/notebooks/Urrnow2/PPO/PPO_my_env_8a337_00000_0_2023-08-19_11-35-47/checkpoint_000125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import gymnasium as gym\n",
    "    gymnasium = True\n",
    "except Exception:\n",
    "    import gym\n",
    "\n",
    "    gymnasium = False\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "env_name = \"my_env\"\n",
    "env = GoLeftEnv()\n",
    "#algo = PPOConfig().environment(env_name).rollouts(num_rollout_workers=1).build()\n",
    "iii = 0\n",
    "while iii<100:\n",
    "  iii = iii+1\n",
    "  episode_reward = 0\n",
    "  terminated = truncated = False\n",
    "\n",
    "  if gymnasium:\n",
    "      obs, info = env.reset()\n",
    "  else:\n",
    "      obs = env.reset()\n",
    "\n",
    "  while not terminated and not truncated:\n",
    "      action = algo2.compute_single_action(obs)\n",
    "      if gymnasium:\n",
    "          obs, reward, terminated, truncated, info = env.step(action)\n",
    "      else:\n",
    "          obs, reward, terminated, info = env.step(action)\n",
    "      episode_reward += reward\n",
    "  print(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
